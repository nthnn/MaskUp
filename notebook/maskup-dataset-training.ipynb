{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d475da26",
   "metadata": {},
   "source": [
    "# Face Mask Detection Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dd315",
   "metadata": {},
   "source": [
    "Importing important libraries. Proceed if you have the following modules installed already, otherwise:\n",
    "\n",
    "```batch\n",
    "py -m pip install numpy opencv-python keras sklearn tensorflow imutils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import h5py\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccc46a",
   "metadata": {},
   "source": [
    "This convolutional network utilizes a combination of two Convolutional and MaxPooling layers to extract features from the dataset. Subsequently, a Flatten layer is employed to convert the data into a one-dimensional format, followed by a Dropout layer to mitigate overfitting.\n",
    "\n",
    "Finally, the network incorporates two Dense layers for classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5342771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b458dc",
   "metadata": {},
   "source": [
    "The generation and augmentation of the image data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e66964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n",
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('../dataset/train', batch_size=10, target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "validation_generator = validation_datagen.flow_from_directory('../dataset/test', batch_size=10, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dccc85",
   "metadata": {},
   "source": [
    "Initialize a callback checkpoint that saves the best model after each epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccce7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../models/epoch/model-{epoch:03d}.model',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa893268",
   "metadata": {},
   "source": [
    "Actual model training and fit generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27583aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_8648\\3620006576.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - ETA: 0s - loss: 0.4922 - acc: 0.7506INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 70s 527ms/step - loss: 0.4922 - acc: 0.7506 - val_loss: 0.2159 - val_acc: 0.9227\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2843 - acc: 0.8897INFO:tensorflow:Assets written to: ../models/epoch\\model-002.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-002.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 61s 460ms/step - loss: 0.2843 - acc: 0.8897 - val_loss: 0.1251 - val_acc: 0.9485\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2987 - acc: 0.8821INFO:tensorflow:Assets written to: ../models/epoch\\model-003.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-003.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 59s 445ms/step - loss: 0.2987 - acc: 0.8821 - val_loss: 0.0989 - val_acc: 0.9639\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2542 - acc: 0.9133INFO:tensorflow:Assets written to: ../models/epoch\\model-004.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-004.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 64s 486ms/step - loss: 0.2542 - acc: 0.9133 - val_loss: 0.0663 - val_acc: 0.9794\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 64s 486ms/step - loss: 0.2615 - acc: 0.9034 - val_loss: 0.1495 - val_acc: 0.9381\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 65s 493ms/step - loss: 0.2277 - acc: 0.9141 - val_loss: 0.0934 - val_acc: 0.9742\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2126 - acc: 0.9232INFO:tensorflow:Assets written to: ../models/epoch\\model-007.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-007.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 66s 499ms/step - loss: 0.2126 - acc: 0.9232 - val_loss: 0.0290 - val_acc: 0.9948\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 64s 488ms/step - loss: 0.2023 - acc: 0.9262 - val_loss: 0.0738 - val_acc: 0.9639\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 67s 508ms/step - loss: 0.1783 - acc: 0.9376 - val_loss: 0.0422 - val_acc: 0.9897\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 71s 540ms/step - loss: 0.1595 - acc: 0.9346 - val_loss: 0.2257 - val_acc: 0.8918\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56736449",
   "metadata": {},
   "source": [
    "Now, save the model to a file that will be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c500dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/keras_model/maskup-model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
