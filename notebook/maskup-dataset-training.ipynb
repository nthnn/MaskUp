{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d475da26",
   "metadata": {},
   "source": [
    "# Face Mask Detection Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dd315",
   "metadata": {},
   "source": [
    "Importing important libraries. Proceed if you have the following modules installed already, otherwise:\n",
    "\n",
    "```batch\n",
    "py -m pip install numpy opencv-python keras sklearn tensorflow imutils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import h5py\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccc46a",
   "metadata": {},
   "source": [
    "This convolutional network utilizes a combination of two Convolutional and MaxPooling layers to extract features from the dataset. Subsequently, a Flatten layer is employed to convert the data into a one-dimensional format, followed by a Dropout layer to mitigate overfitting.\n",
    "\n",
    "Finally, the network incorporates two Dense layers for classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5342771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(100, (3, 3), activation = 'relu', input_shape = (150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(100, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(50, activation = 'relu'),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b458dc",
   "metadata": {},
   "source": [
    "The generation and augmentation of the image data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e66964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n",
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1.0 / 255,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode = 'nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('../dataset/train', batch_size = 10, target_size = (150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1.0 / 255)\n",
    "validation_generator = validation_datagen.flow_from_directory('../dataset/test', batch_size = 10, target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dccc85",
   "metadata": {},
   "source": [
    "Initialize a callback checkpoint that saves the best model after each epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccce7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../models/epoch/model-{epoch:03d}.model',\n",
    "                             monitor = 'val_loss',\n",
    "                             verbose = 0,\n",
    "                             save_best_only = True,\n",
    "                             mode = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa893268",
   "metadata": {},
   "source": [
    "Actual model training and fit generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27583aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_19032\\3361117740.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - ETA: 0s - loss: 0.6335 - acc: 0.6091INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 64s 476ms/step - loss: 0.6335 - acc: 0.6091 - val_loss: 0.6925 - val_acc: 0.5052\n",
      "Epoch 2/30\n",
      "132/132 [==============================] - 64s 482ms/step - loss: 0.6936 - acc: 0.4973 - val_loss: 0.6929 - val_acc: 0.5052\n",
      "Epoch 3/30\n",
      "132/132 [==============================] - 70s 532ms/step - loss: 0.6937 - acc: 0.5087 - val_loss: 0.6938 - val_acc: 0.4897\n",
      "Epoch 4/30\n",
      "132/132 [==============================] - 64s 483ms/step - loss: 0.6922 - acc: 0.4837 - val_loss: 0.6938 - val_acc: 0.4845\n",
      "Epoch 5/30\n",
      "132/132 [==============================] - 62s 472ms/step - loss: 0.6918 - acc: 0.5156 - val_loss: 0.6971 - val_acc: 0.5206\n",
      "Epoch 6/30\n",
      "132/132 [==============================] - 57s 431ms/step - loss: 0.6929 - acc: 0.5034 - val_loss: 0.6935 - val_acc: 0.4794\n",
      "Epoch 7/30\n",
      "132/132 [==============================] - 60s 454ms/step - loss: 0.6911 - acc: 0.5171 - val_loss: 0.7019 - val_acc: 0.4742\n",
      "Epoch 8/30\n",
      "132/132 [==============================] - 74s 564ms/step - loss: 0.6930 - acc: 0.5247 - val_loss: 0.7570 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.6505 - acc: 0.6380INFO:tensorflow:Assets written to: ../models/epoch\\model-009.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-009.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 68s 511ms/step - loss: 0.6505 - acc: 0.6380 - val_loss: 0.5189 - val_acc: 0.7732\n",
      "Epoch 10/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5837 - acc: 0.7004INFO:tensorflow:Assets written to: ../models/epoch\\model-010.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-010.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 65s 491ms/step - loss: 0.5837 - acc: 0.7004 - val_loss: 0.2033 - val_acc: 0.9124\n",
      "Epoch 11/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.3624 - acc: 0.8555INFO:tensorflow:Assets written to: ../models/epoch\\model-011.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-011.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 82s 621ms/step - loss: 0.3624 - acc: 0.8555 - val_loss: 0.1051 - val_acc: 0.9536\n",
      "Epoch 12/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2739 - acc: 0.9110INFO:tensorflow:Assets written to: ../models/epoch\\model-012.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-012.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 72s 546ms/step - loss: 0.2739 - acc: 0.9110 - val_loss: 0.0999 - val_acc: 0.9691\n",
      "Epoch 13/30\n",
      "132/132 [==============================] - 72s 545ms/step - loss: 0.2872 - acc: 0.8890 - val_loss: 0.1072 - val_acc: 0.9742\n",
      "Epoch 14/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2259 - acc: 0.9202INFO:tensorflow:Assets written to: ../models/epoch\\model-014.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-014.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 64s 481ms/step - loss: 0.2259 - acc: 0.9202 - val_loss: 0.0750 - val_acc: 0.9691\n",
      "Epoch 15/30\n",
      "132/132 [==============================] - 63s 480ms/step - loss: 0.2309 - acc: 0.9148 - val_loss: 0.1542 - val_acc: 0.9124\n",
      "Epoch 16/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1929 - acc: 0.9285INFO:tensorflow:Assets written to: ../models/epoch\\model-016.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-016.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 78s 592ms/step - loss: 0.1929 - acc: 0.9285 - val_loss: 0.0691 - val_acc: 0.9897\n",
      "Epoch 17/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1771 - acc: 0.9354INFO:tensorflow:Assets written to: ../models/epoch\\model-017.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-017.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 76s 576ms/step - loss: 0.1771 - acc: 0.9354 - val_loss: 0.0576 - val_acc: 0.9742\n",
      "Epoch 18/30\n",
      "132/132 [==============================] - 69s 519ms/step - loss: 0.1698 - acc: 0.9331 - val_loss: 0.0620 - val_acc: 0.9742\n",
      "Epoch 19/30\n",
      "132/132 [==============================] - 69s 524ms/step - loss: 0.1543 - acc: 0.9331 - val_loss: 0.0848 - val_acc: 0.9588\n",
      "Epoch 20/30\n",
      "132/132 [==============================] - 59s 445ms/step - loss: 0.1650 - acc: 0.9399 - val_loss: 0.0800 - val_acc: 0.9691\n",
      "Epoch 21/30\n",
      "132/132 [==============================] - 58s 436ms/step - loss: 0.1503 - acc: 0.9437 - val_loss: 0.0586 - val_acc: 0.9742\n",
      "Epoch 22/30\n",
      "132/132 [==============================] - 69s 526ms/step - loss: 0.1363 - acc: 0.9529 - val_loss: 0.1367 - val_acc: 0.9433\n",
      "Epoch 23/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1633 - acc: 0.9468INFO:tensorflow:Assets written to: ../models/epoch\\model-023.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-023.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 61s 460ms/step - loss: 0.1633 - acc: 0.9468 - val_loss: 0.0450 - val_acc: 0.9742\n",
      "Epoch 24/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1470 - acc: 0.9498INFO:tensorflow:Assets written to: ../models/epoch\\model-024.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-024.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 68s 512ms/step - loss: 0.1470 - acc: 0.9498 - val_loss: 0.0180 - val_acc: 0.9948\n",
      "Epoch 25/30\n",
      "132/132 [==============================] - 61s 461ms/step - loss: 0.1310 - acc: 0.9506 - val_loss: 0.0498 - val_acc: 0.9794\n",
      "Epoch 26/30\n",
      "132/132 [==============================] - 71s 537ms/step - loss: 0.1371 - acc: 0.9529 - val_loss: 0.0312 - val_acc: 0.9897\n",
      "Epoch 27/30\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1179 - acc: 0.9551INFO:tensorflow:Assets written to: ../models/epoch\\model-027.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-027.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 74s 559ms/step - loss: 0.1179 - acc: 0.9551 - val_loss: 0.0161 - val_acc: 0.9948\n",
      "Epoch 28/30\n",
      "132/132 [==============================] - 92s 694ms/step - loss: 0.1243 - acc: 0.9536 - val_loss: 0.0173 - val_acc: 0.9897\n",
      "Epoch 29/30\n",
      "132/132 [==============================] - 73s 553ms/step - loss: 0.1153 - acc: 0.9506 - val_loss: 0.0300 - val_acc: 0.9897\n",
      "Epoch 30/30\n",
      "132/132 [==============================] - 61s 463ms/step - loss: 0.1262 - acc: 0.9574 - val_loss: 0.0336 - val_acc: 0.9948\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56736449",
   "metadata": {},
   "source": [
    "Now, save the model to a file that will be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
