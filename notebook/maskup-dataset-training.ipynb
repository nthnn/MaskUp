{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d475da26",
   "metadata": {},
   "source": [
    "# Face Mask Detection Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dd315",
   "metadata": {},
   "source": [
    "Importing important libraries. Proceed if you have the following modules installed already, otherwise:\n",
    "\n",
    "```batch\n",
    "py -m pip install numpy opencv-python keras sklearn tensorflow imutils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import h5py\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d0dbb",
   "metadata": {},
   "source": [
    "This convolutional network utilizes a combination of two Convolutional and MaxPooling layers to extract features from the dataset. Subsequently, a Flatten layer is employed to convert the data into a one-dimensional format, followed by a Dropout layer to mitigate overfitting.\n",
    "\n",
    "Finally, the network incorporates two Dense layers for classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5342771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97799aa",
   "metadata": {},
   "source": [
    "The generation and augmentation of the image data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e66964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n",
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('../dataset/train', batch_size=10, target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "validation_generator = validation_datagen.flow_from_directory('../dataset/test', batch_size=10, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec62ef7",
   "metadata": {},
   "source": [
    "Initialize a callback checkpoint that saves the best model after each epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccce7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../models/epoch/model-{epoch:03d}.model',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a837c",
   "metadata": {},
   "source": [
    "Actual model training and fit generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27583aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_10996\\3620006576.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - ETA: 0s - loss: 0.5400 - acc: 0.7323INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-001.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 66s 489ms/step - loss: 0.5400 - acc: 0.7323 - val_loss: 0.1609 - val_acc: 0.9381\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 63s 474ms/step - loss: 0.3170 - acc: 0.8677 - val_loss: 0.2330 - val_acc: 0.9433\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.3280 - acc: 0.8814INFO:tensorflow:Assets written to: ../models/epoch\\model-003.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-003.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 64s 483ms/step - loss: 0.3280 - acc: 0.8814 - val_loss: 0.1607 - val_acc: 0.9330\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2501 - acc: 0.9087INFO:tensorflow:Assets written to: ../models/epoch\\model-004.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-004.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 63s 477ms/step - loss: 0.2501 - acc: 0.9087 - val_loss: 0.0911 - val_acc: 0.9639\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 63s 472ms/step - loss: 0.2144 - acc: 0.9194 - val_loss: 0.1719 - val_acc: 0.9278\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2201 - acc: 0.9110INFO:tensorflow:Assets written to: ../models/epoch\\model-006.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/epoch\\model-006.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 62s 467ms/step - loss: 0.2201 - acc: 0.9110 - val_loss: 0.0701 - val_acc: 0.9742\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 61s 463ms/step - loss: 0.2078 - acc: 0.9247 - val_loss: 0.0772 - val_acc: 0.9742\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 61s 461ms/step - loss: 0.1915 - acc: 0.9270 - val_loss: 0.0913 - val_acc: 0.9691\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 61s 461ms/step - loss: 0.1783 - acc: 0.9323 - val_loss: 0.1149 - val_acc: 0.9588\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 62s 468ms/step - loss: 0.1959 - acc: 0.9331 - val_loss: 0.1138 - val_acc: 0.9485\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28a3ff",
   "metadata": {},
   "source": [
    "Now, save the model to a file that will be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c500dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/keras_model/maskup-model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
